<h1 id="introduction">Introduction</h1>
<p>Support Vector Machines (SVMs) are one of the most popular supervised learning models today, able to perform both linear and nonlinear classification.</p>
<h1 id="linear-classification">Linear Classification</h1>
<p>The idea behind SVMs is to maximize the margin, the distance between the hyperplane (decision boundary) and the samples nearest to this hyperplane, called support vectors.</p>
<div class="figure">
<img src="svm.jpg" alt="Support Vector Machine" />
<p class="caption">Support Vector Machine<span data-label="fig:svm"></span></p>
</div>
<p>The decision boundaries to the left separate the training data correctly but would not generalize well to unseen data, being too close to the training samples (i.e. having a small margin). On the other hand, the decision boundary to the right marked by the dashed line separates the training data and generalizes well to unseen data, having a large margin. Maximization of the margin allows for the least generalization error.</p>
<p><span class="math inline">\(\bm{w}\)</span> is defined as a vector normal to the decision boundary. The positive hyperplane is defined as <span class="math display">\[\bm{w \cdot x_{pos}} + w_0 = 1\]</span> while the negative hyperplane is: <span class="math display">\[\bm{w \cdot x_{neg}} + w_0 = -1\]</span></p>
<p>We can combine these equations by subtracting the second equation from the first: <span class="math display">\[\label{eq:hyperplanes}
\bm{w}{(x_{pos} - x_{neg})} = 2\]</span></p>
<p>To calculate the margin, first, let us take the difference between a positive support vector and a negative support vector.</p>
<p><span class="math display">\[x_{pos} - x_{neg}\]</span></p>
<p>Then, we need to multiply this by a unit vector perpendicular to the hyperplanes. We earlier defined <span class="math inline">\(\bm{w}\)</span> to be normal to the hyperplanes, so <span class="math inline">\(\frac{\bm{w}}{||\bm{w}||}\)</span> serves this purpose:</p>
<p><span class="math display">\[\frac{\bm{w}(x_{pos} - x_{neg})}{||\bm{w}||}\]</span></p>
<p>Using Â [eq:hyperplanes], we arrive at:</p>
<p><span class="math display">\[\frac{\bm{w}(x_{pos} - x_{neg})}{||\bm{w}||} = \frac{2}{||\bm{w}||}\]</span></p>
<p>We must maximize <span class="math inline">\(\frac{2}{||\bm{w}||}\)</span> to maximize the margin. For mathematical convenience, we can minimize <span class="math inline">\(\frac{1}{2}{||\bm{w}||^2}\)</span> to achieve the same effect. The constraint for this optimization problem is that the samples are actually classified correctly:</p>
<p><span class="math display">\[\bm{w \cdot x_{i}} + w_0 \geq 1 \text{ if $y_i = 1$}\]</span></p>
<p><span class="math display">\[\bm{w \cdot x_{i}} + w_0 &lt; -1 \text{ if $y_i = -1$}\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is a particular sample and <span class="math inline">\(y_i\)</span> is the class of the sample. More compactly:</p>
<p><span class="math display">\[y_i(w_0 + \bm{w \cdot x_i}) \geq 1\]</span></p>
<p>After maximizing the margin, our decision rule is:</p>
<p><span class="math display">\[y_i = sign(\bm{w \cdot x_i} + w_0)\]</span></p>
<p>That is, points to the left of the decision boundary are classified as negative while points to the right are classified as positive.</p>
<h1 id="nonlinear-classification-using-kernels">Nonlinear Classification using Kernels</h1>
<p>In the real world, data is usually not linearly separable, meaning that the support vector machine as cannot accurately separate the data. However, we can project the data onto a higher dimensional space where the data is linearly separable using a mapping function <span class="math inline">\(\phi{(\cdot)}\)</span> For example: <span class="math display">\[\phi{(x_1, x_2)} = (z_1, z_2, z_3) = (x_1, x_2, x_1^2 + x_2^2)\]</span> Using this mapping function allows us to separate the two classes below (indicated by red and blue) with a linear hyperplane. We can then project this back into two-dimensional space where the decision boundary becomes nonlinear.</p>
<div class="figure">
<img src="dimensions.jpg" alt="Projecting to higher space" />
<p class="caption">Projecting to higher space<span data-label="fig:mapping"></span></p>
</div>
<p>The problem, however, with this approach is its efficiency. When solving the optimization problem of maximizing the margin, the pair-wise dot products of different training samples <span class="math inline">\(\bm{x_i}\)</span> and <span class="math inline">\(\bm{x_j}\)</span> must be calculated, a very computationally expensive process in high-dimensional space. To solve this, we can use the kernel trick; we can use kernel functions to implicitly calculate the dot product of <span class="math inline">\(\bm{x_i}\)</span> and <span class="math inline">\(\bm{x_j}\)</span> without explicitly projecting them into higher dimensional space.</p>
<p>One of the most popular kernel functions is the Radial Basis Function kernel (RBF kernel) or Gaussian kernel:</p>
<p><span class="math display">\[k(\bm{x_i}, \bm{x_j}) = \exp{(-\gamma||\bm{x_i}-\bm{x_j}||^2})\]</span></p>
<p><span class="math inline">\(\gamma\)</span> is a free parameter that can be optimized.</p>
