<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Generative Adversarial Networks | TJHSST Machine Learning Club</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../css/demo.css" />
    <link rel="stylesheet" type="text/css" href="../../css/component.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../manifest.json">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#5bbad5">
    <meta name="theme-color" content="#ffffff">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-105333430-1', 'auto');
      ga('send', 'pageview');

    </script>
  </head>
  <body>
    <div class="container">
        <h2 style="text-align:center">TJHSST Machine Learning</h2>
         <section class="section section--menu" id="Alonso">
             <span class="link-copy"></span>
             <nav class="menu menu--alonso">
                 <ul class="menu__list">
                     <li class="menu__item"><a href="../../index.html" class="menu__link">Home</a></li>
                     <li class="menu__item menu__item--current"><a href="../../schedule.html" class="menu__link">Lectures</a></li>
                     <li class="menu__item "><a href="../../rankings.html" class="menu__link">Rankings</a></li>
                     <li class="menu__item"><a href="../../resources.html" class="menu__link">Resources</a></li>
                     <li class="menu__item"><a href="../../projects.html" class="menu__link">Projects</a></li>
                     <li class="menu__item"><a href="../../submit.html" class="menu__link">Updates</a></li>
                     <li class="menu__line"></li>
                 </ul>
             </nav>
         </section>
    </div>
    <section class="main-content">
        <h2 style="text-align:center; color:#000">Generative Adversarial Networks</h2>
        <h3 style="text-align:center; color:#000">Justin Zhang</h2>
        <h3 style="text-align:center; color:#000">March 2018</h2>

          <h1 id="introduction">Introduction</h1>
          <blockquote>Adversarial training is the coolest thing since sliced bread.
            <footer>— Yann Lecunn</footer>
          </blockquote>
          <p>Generative Adversarial Networks (GANs) are a seminal type of generative model, introduced in 2014 by the University of Montreal. GANs have been heavily used in various generative tasks with impressive results. GANs are most actively used for image generation tasks: plain image generation, image inpainting, super-resolution image generation, and text-to-image. However, they are beginning to see some use in other types of inputs (e.g. audio, video, etc). Since GAN’s first release, there have been multiple iterations on different types of GANs; here, we’ll cover the basics only.</p>
          <h1 id="intuition">Intuition</h1>
          <p>GANs consist of two neural networks: the generator and the discriminator. The generator is tasked with generating fake samples from random noise. The discriminator is tasked with distinguishing fake samples with real ones (from the dataset).</p>
          <div class="figure">
          <img src="gan.png" alt="The GAN pipeline." />
          <p class="caption">The GAN pipeline.<span data-label="fig:gan"></span></p>
          </div>
          <p>In this way, the two networks play a game of cat-and-mouse; they each try to beat the other. Thoughout training, the goal is to have both networks simultaneously improve: the generator increasingly generates more convincing images to fool the discriminator, and the discriminator becomes better at separating real and fake samples as a result.</p>
          <h1 id="training-specifics">Training specifics</h1>
          <p>With regards to images, usually deep convolutional layers are used, for both the discriminator and the generator.</p>
          <div class="figure">
          <img src="dcgan.png" alt="The DCGAN pipeline." />
          <p class="caption">The DCGAN pipeline.<span data-label="fig:dcgan"></span></p>
          </div>
          <p>Here, deconvolutional layers are used in the generator to produce the target <span class="math inline">\(64\)</span> by <span class="math inline">\(64\)</span> image. This image is then fed into the discriminator for classification.</p>
          <p>A typical training pipeline would be to randomly initialize the two networks. Then, we would iterate over the dataset. At each iteration, we would generate the batch size number of fake images from the generator, so that we would have an equal number of fake and real images. Then, we could train the discriminator on these images with a binary cross-entropy loss. Finally, imagine the generator and discriminator to be one large network, going from noise (via uniform or gaussian) as input into the generator and discriminator to the binary output. In the perspective of the generator, we want to output the incorrect labels (i.e. always <span class="math inline">\(1\)</span>, the output is the probability of the input to the discriminator being a real image) in this concatenated network. In this way, we can freeze the discriminator weights and flow the gradients through it to train the generator. We can then repeat this for each batch, over a number of epochs, as per usual.</p>
          <p>Note that GANs are notoriously difficult to train. This is because GANs are highly unstable; in order to train correctly, we need the generator and discriminator to be roughly on equal levels throughout the training process. If the discriminator overpowers the generator, there will be little gradient for the generator to learn upon; vice-versa, and we run into <em>mode collapse</em>, where the generator produces outputs with extremely low variety.</p>
          <h1 id="math">Math</h1>
          <p>The gradient expression we train the discriminator on is as follows: <span class="math display">\[\nabla \frac{1}{m} \sum_{i=1}^m [\log D(x_i) + \log(1-D(G(z_i))]\]</span> where <span class="math inline">\(x\)</span> is the real data in a given batch, <span class="math inline">\(z\)</span> is the noise for the generator for the given batch, <span class="math inline">\(D\)</span> is the discriminator function, and <span class="math inline">\(G\)</span> is the generator. We want to maximize this expression. The first expression in the summation, <span class="math inline">\(\log D(x_i)\)</span>, corresponds to the discriminator output on real data; we clearly want to maximize this probability. The second is a bit more complicated: the <span class="math inline">\(D(G(z_i))\)</span> corresponds to the discriminator’s probability estimate that the generated data is real. We want to minimize this, so we make the term <span class="math inline">\(\log (1-D(G(z_i))\)</span>. If you haven’t taken multivariable calculus yet, think about the <span class="math inline">\(\nabla\)</span> as a derivative; we simply want to move in the direction that maximizes the summation.</p>
          <p>The gradient expression we train the generator on is as follows: <span class="math display">\[\nabla \frac{1}{m} \sum_{i=1}^m \log(1-D(G(z_i))\]</span> We want to minimize this expression (i.e. we want to maximize <span class="math inline">\(D(G(z_i))\)</span>, the probability of the generated data being real, as determined by the discriminator).</p>
          <h1 id="conclusion">Conclusion</h1>
          <p>GANs are incredibly powerful models; however, training them can be very difficult. GANs are continuously being improved to increase stability and to accommodate different types of input. Stay tuned for Nikhil’s lecture next week on newer GANs!<br />
          For more tips on training a GAN, see <a href="https://github.com/soumith/ganhacks">this</a> link.<br />
          For an example of a GAN, see <a href="https://github.com/jacobgil/keras-dcgan/blob/master/dcgan.py">this</a> link.<br />
          GAN image from <a href="http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html">here</a>.<br />
          DCGAN image from <a href="http://www.timzhangyuxuan.com/project_dcgan/">here</a>.</p>

        <p><a href="../../schedule.html">&larr; Back to lectures</a>
    </section>
  </body>
</html>
